{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a8fd165",
   "metadata": {},
   "source": [
    "## Step 1: Install & Import Dependencies\n",
    "**Documentation**: First, we need to ensure the necessary libraries are installed. We rely heavily on selenium for browser automation.\n",
    "1. selenium: Controls the Chrome browser.\n",
    "2. logging: Helps us track progress and errors.\n",
    "3. pathlib: Creates cross-platform file paths (Windows/Mac/Linux)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef89cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install selenium if you haven't already\n",
    "# !pip install selenium\n",
    "\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException\n",
    "\n",
    "# Configure logging to show timestamps and messages\n",
    "# force=True ensures it reloads config if Jupyter has already started logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(message)s',\n",
    "    datefmt='%H:%M:%S',\n",
    "    handlers=[logging.StreamHandler()],\n",
    "    force=True \n",
    ")\n",
    "\n",
    "print(\" The needed Libraries have been imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5727fe93",
   "metadata": {},
   "source": [
    "## Step 2: Define the Scraper Class\n",
    "**Documentation**:\n",
    "This is the core engine. I have encapsulated all the logic into the StrataScratchScraper class.\n",
    "Key Features included here: <br>\n",
    "2.1. restart_driver(): This is called automatically to prevent the \"Invalid Session ID\" crash by clearing browser memory. <br>\n",
    "2.2.scrape_question_details(): Contains the Accordion Fix. It finds the \"More about this question\" button, scrolls to it, and clicks it to reveal the hidden Obsidian properties (Companies, Job Titles, etc.).<br>\n",
    "2.3. create_markdown(): Formats the data into a clean Markdown file with a YAML frontmatter block for Obsidian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253cbcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrataScratchScraper:\n",
    "    def __init__(self, output_dir: str = \"StrataScratch_Full_DB\", headless: bool = True):\n",
    "        self.base_url = \"https://platform.stratascratch.com\"\n",
    "        self.questions_url = f\"{self.base_url}/coding?code_type=1\"\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.headless = headless\n",
    "        \n",
    "        # Internal state\n",
    "        self.driver = None\n",
    "        self.wait = None\n",
    "        \n",
    "        # Initialize the first driver\n",
    "        self.restart_driver()\n",
    "    \n",
    "    def restart_driver(self):\n",
    "        \"\"\"Closes current driver and starts a fresh one to clear memory.\"\"\"\n",
    "        if self.driver:\n",
    "            try: self.driver.quit()\n",
    "            except: pass\n",
    "            logging.info(\" ‚ö°Restarting Chrome Driver because there is more to life than getting a deadlock...\")\n",
    "            time.sleep(2)\n",
    "\n",
    "        chrome_options = Options()\n",
    "        if self.headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        # Settings to ensure stability and visibility of elements\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "        \n",
    "        self.driver = webdriver.Chrome(options=chrome_options)\n",
    "        self.wait = WebDriverWait(self.driver, 20)\n",
    "    \n",
    "    def slugify(self, text: str) -> str:\n",
    "        \"\"\"Helper to make safe filenames\"\"\"\n",
    "        if not text: return f\"question-{int(time.time())}\"\n",
    "        text = re.sub(r'[\\(\\[].*?[\\)\\]]', '', text).lower()\n",
    "        text = re.sub(r'[^a-z0-9]+', '-', text).strip('-')\n",
    "        return text[:200]\n",
    "\n",
    "    def get_total_pages(self) -> int:\n",
    "        \"\"\"Detects the total number of pages in the pagination bar.\"\"\"\n",
    "        logging.info(\"üîé Detecting total number of pages...\")\n",
    "        try:\n",
    "            self.driver.get(f\"{self.questions_url}&page=1\")\n",
    "            self.wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "            time.sleep(5) \n",
    "            \n",
    "            # Find generic pagination buttons (usually just numbers)\n",
    "            buttons = self.driver.find_elements(By.XPATH, \"//button[string-length(text()) < 3]\")\n",
    "            page_nums = [int(b.text.strip()) for b in buttons if b.text.strip().isdigit()]\n",
    "            \n",
    "            if page_nums:\n",
    "                max_page = max(page_nums)\n",
    "                logging.info(f\"‚úÖ Detected {max_page} pages.\")\n",
    "                return max_page\n",
    "            return 15 # Fallback\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"‚ö†Ô∏è Error detecting pages: {e}. Defaulting to 15.\")\n",
    "            return 15\n",
    "\n",
    "    def extract_question_links(self, page_num: int) -> List[str]:\n",
    "        \"\"\"Extracts question URLs from a listing page with retry logic.\"\"\"\n",
    "        url = f\"{self.questions_url}&page={page_num}\"\n",
    "        for attempt in range(2):\n",
    "            try:\n",
    "                self.driver.get(url)\n",
    "                try:\n",
    "                    self.wait.until(EC.presence_of_element_located((By.XPATH, \"//a[contains(@href, '/coding/')]\")))\n",
    "                except TimeoutException: pass\n",
    "                \n",
    "                time.sleep(4) # Wait for React render\n",
    "                \n",
    "                links = self.driver.find_elements(By.CSS_SELECTOR, \"a[href*='/coding/']\")\n",
    "                valid_links = list(set([l.get_attribute('href') for l in links if '/coding/' in l.get_attribute('href')]))\n",
    "                \n",
    "                if valid_links: return valid_links\n",
    "                \n",
    "                logging.warning(f\"   ‚ö†Ô∏è Page {page_num} returned 0 links (Attempt {attempt+1}). Refreshing...\")\n",
    "                time.sleep(2)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"   Error on page {page_num}: {e}\")\n",
    "        return []\n",
    "\n",
    "    def scrape_question_details(self, url: str) -> Optional[Dict]:\n",
    "        \"\"\"Scrapes detailed data including hidden properties.\"\"\"\n",
    "        self.driver.get(url)\n",
    "        self.wait.until(EC.presence_of_element_located((By.TAG_NAME, \"h1\")))\n",
    "        \n",
    "        # --- THE ACCORDION FIX ---\n",
    "        try:\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(0.5)\n",
    "            accordion_btn = self.driver.find_element(By.XPATH, \"//button[contains(., 'More about this question')]\")\n",
    "            if accordion_btn.get_attribute(\"aria-expanded\") != \"true\":\n",
    "                self.driver.execute_script(\"arguments[0].click();\", accordion_btn)\n",
    "                time.sleep(1)\n",
    "        except: pass # Ignore if button is missing (some questions might not have it)\n",
    "\n",
    "        # --- DATA EXTRACTION ---\n",
    "        title = self.driver.find_element(By.TAG_NAME, \"h1\").text.strip()\n",
    "        \n",
    "        try: q_id = self.driver.find_element(By.XPATH, \"//span[contains(text(), 'ID')]\").text.replace(\"ID\", \"\").strip()\n",
    "        except: q_id = \"Unknown\"\n",
    "\n",
    "        try:\n",
    "            diff_elem = self.driver.find_element(By.CSS_SELECTOR, \"[class*='QuestionDifficulty--']\")\n",
    "            difficulty = \"Unknown\"\n",
    "            if \"Easy\" in diff_elem.text: difficulty = \"Easy\"\n",
    "            elif \"Medium\" in diff_elem.text: difficulty = \"Medium\"\n",
    "            elif \"Hard\" in diff_elem.text: difficulty = \"Hard\"\n",
    "        except: difficulty = \"Unknown\"\n",
    "\n",
    "        try: question_text = self.driver.find_element(By.CLASS_NAME, \"QuestionMetadata__question\").text.strip()\n",
    "        except: question_text = \"\"\n",
    "\n",
    "        # Extract Pills (Companies, Topics)\n",
    "        def get_pills(header):\n",
    "            try:\n",
    "                xpath = f\"//h3[contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), '{header.lower()}')]/following-sibling::div[1]//div[contains(@class, 'QuestionMetadata__pill')]\"\n",
    "                elements = self.driver.find_elements(By.XPATH, xpath)\n",
    "                return [e.text.strip() for e in elements if e.text.strip()]\n",
    "            except: return []\n",
    "\n",
    "        # Extract Tables\n",
    "        tables = []\n",
    "        try:\n",
    "            t_names = self.driver.find_elements(By.CSS_SELECTOR, \".QuestionPane__question-data--tables\")\n",
    "            t_schemas = self.driver.find_elements(By.CLASS_NAME, \"DatasetTableTypes__container\")\n",
    "            for i, t in enumerate(t_names):\n",
    "                schema = {}\n",
    "                if i < len(t_schemas):\n",
    "                    spans = t_schemas[i].find_elements(By.TAG_NAME, \"span\")\n",
    "                    for k in range(0, len(spans)-1, 2):\n",
    "                        schema[spans[k].text.strip().rstrip(':')] = spans[k+1].text.strip()\n",
    "                tables.append({'name': t.text.strip(), 'schema': schema})\n",
    "        except: pass\n",
    "\n",
    "        return {\n",
    "            'title': title, 'id': q_id, 'difficulty': difficulty,\n",
    "            'question': question_text, 'url': url,\n",
    "            'companies': get_pills(\"Companies\"), \n",
    "            'job_positions': get_pills(\"Job Positions\"),\n",
    "            'topic_family': get_pills(\"Topic Family\"), \n",
    "            'topic_functions': get_pills(\"Topic Functions\"),\n",
    "            'tables': tables\n",
    "        }\n",
    "\n",
    "    def create_markdown(self, data: Dict) -> str:\n",
    "        \"\"\"Generates Obsidian-friendly Markdown.\"\"\"\n",
    "        md = \"---\\n\"\n",
    "        md += f\"id: \\\"{data['id']}\\\"\\n\"\n",
    "        md += f\"title: \\\"{data['title'].replace('\\\"', '')}\\\"\\n\"\n",
    "        md += f\"difficulty: {data['difficulty']}\\n\"\n",
    "        md += f\"url: {data['url']}\\n\"\n",
    "        \n",
    "        def write_list(key, vals):\n",
    "            if not vals: return \"\"\n",
    "            res = f\"{key}:\\n\"\n",
    "            for v in vals: res += f\"  - \\\"{v.replace('\\\"', '\\\\\\\"')}\\\"\\n\"\n",
    "            return res\n",
    "\n",
    "        md += write_list(\"companies\", data['companies'])\n",
    "        md += write_list(\"job_positions\", data['job_positions'])\n",
    "        md += write_list(\"topic_family\", data['topic_family'])\n",
    "        md += write_list(\"topic_functions\", data['topic_functions'])\n",
    "        md += \"---\\n\\n\"\n",
    "        md += f\"# {data['title']}\\n\\n\"\n",
    "        md += \"## Question\\n\" + f\"{data['question']}\\n\\n\"\n",
    "        \n",
    "        if data['tables']:\n",
    "            md += \"## Database Schema\\n\"\n",
    "            for t in data['tables']:\n",
    "                md += f\"### {t['name']}\\n\"\n",
    "                if t['schema']:\n",
    "                    md += \"| Column | Type |\\n|---|---|\\n\"\n",
    "                    for c, d in t['schema'].items(): md += f\"| {c} | {d} |\\n\"\n",
    "                    md += \"\\n\"\n",
    "        return md\n",
    "    \n",
    "    def close(self):\n",
    "        if self.driver: self.driver.quit()\n",
    "\n",
    "print(\" The Class has been defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2b49eb",
   "metadata": {},
   "source": [
    "## Step 3: Initialize the Scraper\n",
    "**Documentation**:\n",
    "Here we set up the output directory.\n",
    "Action: Change output_dir to your preferred folder (or your Obsidian vault path).\n",
    "Action: Keep headless=True for speed, or False if you want to watch it work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4756f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scraper\n",
    "# CHANGE THIS PATH to where you want the files saved\n",
    "OUTPUT_PATH = \"StrataScratch_Notebook_Export\" \n",
    "\n",
    "scraper = StrataScratchScraper(\n",
    "    output_dir=OUTPUT_PATH, \n",
    "    headless=True \n",
    ")\n",
    "\n",
    "print(f\" The Scraper has been initialized. Saving to: {scraper.output_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79248901",
   "metadata": {},
   "source": [
    "## Step 4: Phase 1 - Collect All Links\n",
    "**Documentation**:\n",
    "This step loops through the pages and collects the URLs.<br>\n",
    "It automatically detects if there are 14, 15, or more pages.<br>\n",
    "It iterates through them and collects all unique question links.<br>\n",
    "It removes duplicates.<br>\n",
    "*This takes about 1-2 minutes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b569fb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Detect Total Pages\n",
    "total_pages = scraper.get_total_pages()\n",
    "all_links = []\n",
    "\n",
    "print(f\"Collecting links from {total_pages} pages...\")\n",
    "\n",
    "# 2. Loop through pages\n",
    "for p in range(1, total_pages + 1):\n",
    "    print(f\"   Scanning Page {p}/{total_pages}...\", end=\"\\r\")\n",
    "    links = scraper.extract_question_links(p)\n",
    "    \n",
    "    if links:\n",
    "        all_links.extend(links)\n",
    "    \n",
    "    # Restart driver every 5 pages during collection to ensure valid session\n",
    "    if p % 5 == 0:\n",
    "        scraper.restart_driver()\n",
    "\n",
    "# Remove duplicates\n",
    "all_links = list(set(all_links))\n",
    "\n",
    "print(f\"\\n Good news, Collection Complete. We Found {len(all_links)} unique questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56331c7",
   "metadata": {},
   "source": [
    "## Step 5: Phase 2 - Scrape Data (The Heavy Lifting)\n",
    "Documentation:\n",
    "This is the main loop that visits every single question page.\n",
    "Safety Features Active:\n",
    "1. Memory Management: Every 25 questions, it restarts the browser automatically.\n",
    "2. Crash Recovery: If a specific question crashes the browser (Invalid Session ID), it catches the error, restarts the browser, and retries that question once before moving on.\n",
    "This step will take time (approx 25-35 minutes for 666 questions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0f5f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "print(\" The Actual Scraping Starts now...\")\n",
    "\n",
    "for i, link in enumerate(all_links):\n",
    "    \n",
    "    # --- PREVENTIVE MAINTENANCE ---\n",
    "    # Restart driver every 25 questions to clear RAM\n",
    "    if i > 0 and i % 25 == 0:\n",
    "        scraper.restart_driver()\n",
    "    \n",
    "    # Progress indicator\n",
    "    print(f\"[{i+1}/{len(all_links)}] Processing...\", end=\"\\r\")\n",
    "    \n",
    "    try:\n",
    "        # Attempt 1\n",
    "        data = scraper.scrape_question_details(link)\n",
    "        \n",
    "        if data:\n",
    "            fname = f\"{scraper.slugify(data['title'])}.md\"\n",
    "            with open(scraper.output_dir / fname, 'w', encoding='utf-8') as f:\n",
    "                f.write(scraper.create_markdown(data))\n",
    "            logging.info(f\"[{i+1}] Saved: {fname}\")\n",
    "            scraped_count += 1\n",
    "        else:\n",
    "            failed_count += 1\n",
    "            \n",
    "    except WebDriverException:\n",
    "        # CRASH HANDLER\n",
    "        logging.error(f\"[{i+1}] Oh no the Browser has crashed! Restarting and retrying...\")\n",
    "        scraper.restart_driver()\n",
    "        try:\n",
    "            # Attempt 2 (Retry)\n",
    "            data = scraper.scrape_question_details(link)\n",
    "            if data:\n",
    "                fname = f\"{scraper.slugify(data['title'])}.md\"\n",
    "                with open(scraper.output_dir / fname, 'w', encoding='utf-8') as f:\n",
    "                    f.write(scraper.create_markdown(data))\n",
    "                logging.info(f\"[{i+1}] Saved (after retry): {fname}\")\n",
    "                scraped_count += 1\n",
    "            else:\n",
    "                failed_count += 1\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{i+1}] ‚ùå Retry failed: {e}\")\n",
    "            failed_count += 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"[{i+1}] ‚ùå Error: {e}\")\n",
    "        failed_count += 1\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\" Damn brody we actually Scraped: {scraped_count} | Unfortunately we Failed on: {failed_count}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd435a0a",
   "metadata": {},
   "source": [
    "## Step 6: Cleanup\n",
    "Documentation:\n",
    "Always good practice to close the browser driver when finished to free up system resources.\n",
    "Cleanup is for the weak minded, real men consume resources and blame it on AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea733c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.close()\n",
    "print(\"‚úÖ Driver closed.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
